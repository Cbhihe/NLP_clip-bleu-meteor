{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82452c5",
   "metadata": {},
   "source": [
    "### Problem statementÂ¶\n",
    "\n",
    "This notebook does 3 things:\n",
    "\n",
    "- Experiment with loading the Torch v1.7.1 cpu-only based CLIP v1.0 model, for implementation of one-shot inference engine on annotation-image pairs (testing dataset).\n",
    "- Select images on the condition that their '.xml' metadata specifies at least 3 bbxes.\n",
    "- Classify selected images according to the labels used in training using CLIP one-shot inference. <BR>Results are saved in file: '../Sgoab/Data/Images/clip_lab-class.tsv'\n",
    "\n",
    "_CLIP is the *Contrastive Language-Image Pre-Training* neural network trained on a variety of (image, text) pairs, first reported by Radford et al (2020) and applied by Hessel et al (2021). <BR>\n",
    "In principle, given an image, CLIP can be instructed in natural language to predict the most relevant text snippet, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bde0f4",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "This iPython notebook must be run in a Python virtual environment, running Python v3.7.1. This is a prerequisite so the proper versions of Torch 1.7.1+cpu and TorchVision 0.8.2+cpu can be invoked to run the CLIP 1.0 inference engine on test images. A succint installation description is scripted below for a Linux host, assuming that:\n",
    "\n",
    "- your interactive terminal session shell is `bash` or `sh`.\n",
    "- you already setup a Python 3.7.0 virtual environment, in directory `/path/to/my_directory`.\n",
    "- you know how to handle the command line interface on the terminal.\n",
    "\n",
    "#### Setting up and registering a custom iPython kernel\n",
    "\n",
    "What follows applies to CPU-only constrained installations. For CUDA-enabled machines, refer to `https://github.com/OpenAI/CLIP`.\n",
    "\n",
    "- Assuming you have configured `pyenv` (on your favorite Linux host) to manage multiple Python virtual environments with specific package requirements, choose the directory in which to setup your Python virtual environment and install your iPython kernel utility package `ipykernel`:\n",
    "\n",
    "```\n",
    "      $ cd /path/to/my_directory\n",
    "      $ pyenv local 3.7.1\n",
    "      $ python -m pip install ipykernel  # or \"ipykernel==4.10.0\"\n",
    "```\n",
    "- Install every required packages in the virtual environment directory `/path/to/my_directory` (see following section for that).\n",
    "    \n",
    "```\n",
    "      $ python -m pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "      $ python -m pip install ftfy regex tqdm\n",
    "      $ python -m  pip install git+https://github.com/openai/CLIP.git\n",
    "```\n",
    "\n",
    "- Make the new custom iPython kernel, clip1.0, available in interactive Python sessions:\n",
    "```\n",
    "      $ cd /path/to/my_directory\n",
    "      $ ipython kernel install --user --name clip1.0 --display-name \"Python3.7.1 (clip1.0)\"     # or\n",
    "      $ python -m ipykernel install --user --name clip1.0 --display-name \"Python3.7.1 (clip1.0)\"\n",
    "      $ jupyter notebook        # launch an iPython session based on 'notebook' server \n",
    "```\n",
    "\n",
    "- Select the special virtual environment kernel ***Python 3.7.1 (clip1.0)*** under the `New notebook` button in the top-right region of the browser page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e5644",
   "metadata": {},
   "source": [
    "### Package requirements\n",
    "\n",
    "Package requirements are detailed below. For a quick demo also install `Pillow==8.3.2` and dependencies.\n",
    "\n",
    "- Install all required packages in the virtual environment directory \"/path/to/my_directory\", with:\n",
    "```\n",
    "    $ cd /path/to/my_directory\n",
    "    $ python -m pip freeze <<- 'EOF'\n",
    "                clip @ git+https://github.com/openai/CLIP.git@04f4dc2ca1ed0acc9893bd1a3b526a7e02c4bb10ftfy\n",
    "                Cython==0.29.1\n",
    "                h5py==2.9.0\n",
    "                ftfy==5.5.1\n",
    "                matplotlib==3.0.2\n",
    "                numpy==1.17.3\n",
    "                Pillow==8.3.2\n",
    "                pyyaml==5.1\n",
    "                regex==2021.8.3\n",
    "                requests==2.20.1\n",
    "                torch==1.7.1+cpu\n",
    "                torchaudio==0.7.2\n",
    "                torchvision==0.8.2+cpu\n",
    "                tqdm==4.38.0\n",
    "                scipy==1.2.0\n",
    "                zipfile37==0.1.3\n",
    "    EOF\n",
    "```\n",
    "\n",
    "Jupyter environment requirements include:\n",
    "```\n",
    "                ipykernel==6.6.0\n",
    "                ipython==7.30.1\n",
    "                ipython_genutils==0.2.0\n",
    "                ipywidgets==7.6.5\n",
    "                jupyter_client==7.1.0\n",
    "                jupyter_core==4.9.1\n",
    "                nbclient==0.5.9\n",
    "                nbconvert==6.3.0\n",
    "                nbformat==5.1.3\n",
    "                notebook==5.7.4\n",
    "                traitlets==5.1.1\n",
    "```\n",
    " ... and starting the jupyter notebook from the sytem's jupyter's instance with:\n",
    "```\n",
    "    $ /usr/bin/jupyter notebook 01_clip_cpu_classify\n",
    "```\n",
    "#### Known issues\n",
    "\\- Launching the notebook by relying on the local environment's shims, with:\n",
    "\n",
    "    $ jupyter notebook 01_clip_cpu_classify\n",
    "    \n",
    "may fail under Pyenv with a \"Segmentation fault\".  It is likely an iPython issue related to jupyter. To avoid it, launch either notebook from a more recent python version, and select iyour custom built 3.7.1 iPython kernel from the notebook at first launch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c5143",
   "metadata": {},
   "source": [
    "### Licensing terms and copyright\n",
    "\n",
    "A substantial portion of the following code, is based on CLIP, as originally made available by its authors. CLIP, its use and distribution are protected by the terms and conditions of the MIT license.\n",
    "\n",
    "    Copyright (C) 2021 OpenAI\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this \n",
    "software and associated documentation files (the \"Software\"), to deal in the Software \n",
    "without restriction, including without limitation the rights to use, copy, modify, merge, \n",
    "publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons \n",
    "to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or \n",
    "substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n",
    "BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, \n",
    "DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "=========================================================\n",
    "\n",
    "The pre-processing and wrapper code used to conduct one-shot classification with CLIP are protected. \n",
    "\n",
    "    Copyright (c) 2020, 2021 Cedric Bhihe\n",
    "    \n",
    "The OpenAI software made available in this repository was extended with pre- and post-processing steps. The corresponding code is available for free under the terms of the Gnu GPL License version 3 or later.\n",
    "\n",
    "In short, you are welcome to use and distribute the code of this repo as long as you always quote its author's name and the Licensing terms in full. Consult the licensing terms and conditions in License.md on this repo.\n",
    "\n",
    "=========================================================\n",
    "\n",
    "***To contact the repo owner for any issue, please open an new thread under the [Issues] tab on the source code repo.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbbf465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, csv, time\n",
    "from datetime import datetime as dt \n",
    "\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "import multiprocessing as mp\n",
    "#from multiprocessing.pool import ThreadPool\n",
    "import faulthandler     # Python builtin\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFile\n",
    "\n",
    "import numpy as np\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "faulthandler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d424d5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "## Check the pyTorch version used\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "#assert torch.__version__.split(\".\") >= [\"1\", \"7\", \"1\"], \"PyTorch 1.7.1 or later is re{commend,quir}ed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04a9345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TESTING\n",
    "clip.available_models()   # list names of available CLIP models\n",
    "## TESTING END"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c3bb104",
   "metadata": {},
   "source": [
    "%lsmagic\n",
    "#%%bash\n",
    "#ls -AFl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238f4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Image input resolution: 224 by 224 pixels\n",
      "Textual context length: 77\n",
      "Total vocabulary size: 49408\n",
      "Local device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Import trained CLIP model\n",
    "localdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=localdevice, jit=False) # Must set jit=False for training\n",
    "#model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length   # pixel size of resized square picture side\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Image input resolution:\", input_resolution,\"by\",input_resolution,\"pixels\")\n",
    "print(\"Textual context length:\", context_length)\n",
    "print(\"Total vocabulary size:\", vocab_size)\n",
    "print(\"Local device:\", localdevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298121de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CLIP(\n",
       "   (visual): VisionTransformer(\n",
       "     (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "     (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (transformer): Transformer(\n",
       "       (resblocks): Sequential(\n",
       "         (0): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (1): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (2): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (3): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (4): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (5): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (6): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (7): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (8): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (9): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (10): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (11): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (transformer): Transformer(\n",
       "     (resblocks): Sequential(\n",
       "       (0): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (1): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (2): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (3): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (4): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (5): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (6): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (7): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (8): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (9): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (10): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (11): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (token_embedding): Embedding(49408, 512)\n",
       "   (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " Compose(\n",
       "     Resize(size=224, interpolation=PIL.Image.BICUBIC)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     <function _convert_image_to_rgb at 0x7f443eb15d08>\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load model of choice and return the VisionTransformer (for images) and \n",
    "# Transformer (for text) information. The loaded model is not JIT.\n",
    "clip.load(\"ViT-B/32\", device=localdevice, jit=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e967ffe",
   "metadata": {},
   "source": [
    "## TESTING\n",
    "## Image preprocessing:\n",
    "# This is  second return value of 'clip.load(\"ViT-B/32\", device=localdevice, jit=False))'\n",
    "# (see previous cell's output) contains torchvision Transform that performs image preprocessing\n",
    "#  - normalize image pixel intensity using dataset mean and standard deviation    \n",
    "#  - resize input images\n",
    "#  - center-crop images \n",
    "# to conform to image resolution expected by model\n",
    "preprocess\n",
    "## TESTING END"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d9aef35",
   "metadata": {},
   "source": [
    "## TESTING\n",
    "## Text input w/ case-insensitive tokenizer: clip.tokenize()\n",
    "# CLIP models expect 77 tokens in text summaries. By default outputs \n",
    "# are padded to become 77 tokens long.\n",
    "# Context size (default 77 tokens) is modified by second argument of type integer\n",
    "print(clip.tokenize(\"In the painting an angel in the nude plays the trumpet.\",context_length=77))\n",
    "print(clip.tokenize(\"He told the painting showed the nude angel who reclined on the couch.\",context_length=77))\n",
    "# Output:  word tokens + verb-conjugation tokens + punctuation tokens + 1 sos token [49406] \n",
    "#          + 1 eos token [49407] padded to 77 tokens.\n",
    "## TESTING END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3388393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load labels with which object-detection CNN was trained\n",
    "data_dir = 'Data_git/'\n",
    "res_dir = data_dir + 'Results/'\n",
    "\n",
    "file = 'class_w_categories.csv'\n",
    "infile = data_dir + file\n",
    "\n",
    "# class labels only\n",
    "with open(infile,'r', encoding='utf-8-sig') as fd:\n",
    "    f_csv = csv.reader(fd)\n",
    "    classids = [re.sub(r'\\s*$|^\\s*|\\s+(?=\\s)|\\s*,\\s*.*','',row[0]) for row in f_csv if not re.search(r'^\\s*#|^$', row[0])]\n",
    "    classids = sorted(classids, reverse = False)\n",
    "    class_cnt = len(classids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc4df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminary (generic) for image display with decorations\n",
    "#  ####################\n",
    "path_to_font = '/usr/share/fonts/TTF/times.ttf'    # path is system specific\n",
    "if os.path.exists(path_to_font):\n",
    "#if os.path.isfile(path_to_font):\n",
    "   pass\n",
    "else:\n",
    "    print(' ==> Missing font file or erroneous path to file.')\n",
    "    raise Exception\n",
    "\n",
    "font_size1 = 28\n",
    "font_size2 = 22\n",
    "\n",
    "try:\n",
    "    label_font1 = ImageFont.truetype(path_to_font,font_size1)\n",
    "    label_font2 = ImageFont.truetype(path_to_font,font_size2)\n",
    "except (NameError) as ne:\n",
    "    print(ne + '\\n ==> Missing \\'from PIL import ImageFont\\'\\n If necessary install module PIL/Pillow.')\n",
    "    raise ne\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    print('Unsure about what happened. Probably unsafe to continue; ttf loading interrupted.')\n",
    "    raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8160d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter initialization\n",
    "\n",
    "dataset = 'training'\n",
    "max_img_nbr = 3                  # nbr of images + xml files to be extracted from zip archive\n",
    "min_nbr_bbx = 3                  # minimum nbr of bbxes per sampled file\n",
    "top_labels = 5                   # top label ranking for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d98e358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nbr of image files: 5\n",
      "Total nbr of XML files: 5\n",
      "3 files with 3 or more bbxes were selected at random.\n"
     ]
    }
   ],
   "source": [
    "## Load N random images with minimum number of bbxes\n",
    "\n",
    "cnt = 0\n",
    "extract_file_lst = list()\n",
    "sample_file_label_lst = list()\n",
    "\n",
    "# Load  zipped training files\n",
    "file = dataset + '_set.zip'\n",
    "infile = data_dir + file\n",
    "with ZipFile(infile,'r') as zipped:\n",
    "    img_lst = [f.split(sep='/')[-1] for f in zipped.namelist() if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    img_lst.sort()\n",
    "    xml_lst = [f.split(sep='/')[-1] for f in zipped.namelist() if f.endswith('.xml')]\n",
    "    xml_lst.sort()\n",
    "    print(f\"Total nbr of image files: {len(img_lst)}\\nTotal nbr of XML files: {len(xml_lst)}\")\n",
    "    \n",
    "    try:\n",
    "        # sample file basenames w/o replacement\n",
    "        f_sampled = ['.'.join(f.split('.')[:-1]) for f in random.sample(img_lst, max_img_nbr)] \n",
    "    except ValueError:\n",
    "        f_sampled = ['.'.join(f.split('.')[:-1]) for f in img_lst]\n",
    "    \n",
    "    for f in f_sampled:\n",
    "        label_lst = list()\n",
    "        label_dct = dict()\n",
    "        \n",
    "        f_xml = f + '.xml'\n",
    "        zipped.extract(f_xml,path = data_dir + 'Annots/')\n",
    "        tree = ET.parse(data_dir + 'Annots/' + f_xml) \n",
    "        root = tree.getroot()\n",
    "        obj_idx = [(idx,root[idx][0].text)\n",
    "                   for idx, val in enumerate([child.tag for child in root]) \n",
    "                   if val == 'object']\n",
    "        if len(obj_idx) >= min_nbr_bbx:\n",
    "            f_jpg = f + '.jpg'\n",
    "            zipped.extract(f_jpg,path = data_dir + 'Images/')\n",
    "            cnt +=1\n",
    "            label_lst = [label for _,label in obj_idx]\n",
    "            for label in set(label_lst):\n",
    "                label_dct[label]=label_lst.count(label)\n",
    "            extract_file_lst.append(f)\n",
    "            sample_file_label_lst.append(label_dct)\n",
    "        else:\n",
    "            try:\n",
    "                os.remove(data_dir + 'Annots/' + f_xml)\n",
    "            except(OSError,) as OSe:\n",
    "                print(f\"### invalid file name or inaccessible path for ({f})\")\n",
    "                print(f\"    or arguments with correct type, but rejected by OS.\")\n",
    "                \n",
    "print(f\"{cnt} files with {min_nbr_bbx} or more bbxes were selected at random.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f2f5096",
   "metadata": {},
   "source": [
    "## TESTING\n",
    "print(data_dir)\n",
    "# extract_file_lst = ['00013167', '00008190', '00012652', '00009587', '00009005', '00012710']\n",
    "#extract_file_list = ['00013116', '00005041']\n",
    "print(extract_file_lst, sample_file_label_lst)\n",
    "cnt = len(extract_file_lst)\n",
    "## TESTING END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67877f90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 top label ranking for 3 images in:\n",
      "Data_git/Images/\n",
      "\n",
      " â¢ 00000110.jpg\n",
      "   time: 2.63s\n",
      "         crucifixion: 53.12%\n",
      "               cross: 22.48%\n",
      "              prayer: 6.70%\n",
      "            shepherd: 3.74%\n",
      "     crown of thorns: 2.95%\n",
      "\n",
      " â¢ 00000028.jpg\n",
      "   time: 2.88s\n",
      "         crucifixion: 86.22%\n",
      "               cross: 6.20%\n",
      "               mitre: 1.08%\n",
      "              banner: 0.82%\n",
      "              prayer: 0.74%\n",
      "\n",
      " â¢ 00000103.jpg\n",
      "   time: 2.74s\n",
      "              prayer: 12.67%\n",
      "              judith: 10.37%\n",
      "             chalice: 7.26%\n",
      "               angel: 6.99%\n",
      "                halo: 5.45%\n",
      "\n",
      "==========================================\n",
      "1 new file was label-based classified by CLIP and added to:\n",
      " Data_git/Results/clip_classify.tsv\n"
     ]
    }
   ],
   "source": [
    "## Prepare text input\n",
    "# 'torch.cat' concatenates given sequence of tensors in given dimension (default: row-wise or 'dim=0')\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a painting of a {c}\") for c in classids]).to(localdevice)\n",
    "image_path = data_dir + 'Images/'\n",
    "\n",
    "## Prepare output\n",
    "print(f\"{top_labels} top label ranking for {cnt} images in:\\n{image_path}\")\n",
    "\n",
    "try:\n",
    "    with open(res_dir + 'clip_classify.tsv', 'x', newline='') as fd:  # create if file does not exist\n",
    "        header = ['img_name', 'rec_create_time', 'img_dataset', 'img_top_labels', 'img_labels']\n",
    "        writer = csv.DictWriter(fd, fieldnames=header, delimiter='\\t')\n",
    "        writer.writeheader()\n",
    "except FileExistsError:\n",
    "    pass\n",
    "       \n",
    "with open(res_dir + 'clip_classify.tsv', 'r', newline='') as infile:  \n",
    "    # binary mode (e.g. 'rb') doesn't accept argument [newline='']\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    ncol = len(header)\n",
    "    ndarr = np.empty([0,ncol],dtype='str')            # empty array\n",
    "    for row in reader:\n",
    "        ndarr = np.append(ndarr,[row],axis=0)\n",
    "\n",
    "\n",
    "img_in_file = [str(x) for x in list(ndarr[:,0])]\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "cnt_new = 0\n",
    "\n",
    "for f_idx, f in enumerate(extract_file_lst):\n",
    "    starttime = time.time()\n",
    "    timestamp = dt.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "    f_jpg = image_path + f + '.jpg'\n",
    "    img = Image.open(f_jpg, mode='r',formats=None) # lazy op, default: mode='r',formats=None\n",
    "    width, height = img.size\n",
    "    img_labels = sample_file_label_lst[f_idx]\n",
    "\n",
    "    # Use 'mode=RGB' or 'mode=RGBA' below, to blend drawing in underlying image\n",
    "    draw = ImageDraw.Draw(img, mode='RGBA') \n",
    "\n",
    "    # Modify font size to fit image size\n",
    "    multiplier = max(1,(width+height)//1400)*4/5\n",
    "    label_font1 = ImageFont.truetype(path_to_font,int(font_size1*multiplier))\n",
    "    label_font2 = ImageFont.truetype(path_to_font,int(font_size2*multiplier))\n",
    "\n",
    "    draw.text((width/20,height/20),\n",
    "              'File: ' + str(f_jpg.split('/')[-1]),\n",
    "              #fill=contrasted_rgb_color,\n",
    "              fill=(255,40,0,255),                 # ferrari red (255,40,0,...), full opacity (...,255)\n",
    "              font=label_font1)\n",
    "\n",
    "    ## Prepare image input\n",
    "    # Insert new singleton dimension (size 1) along dimension 0,\n",
    "    # so bidimensional  224x224 pixels preprocessed image tensor becomes tensor of size [1,224,224]  \n",
    "    image_input = preprocess(img).unsqueeze(0).to(localdevice) \n",
    "\n",
    "    # Calculate features\n",
    "    # 'torch.no_grad()', context manager to disable gradient calculation locally over indented block\n",
    "    with torch.no_grad():\n",
    "    #with torch.autograd.no_grad():\n",
    "    #with torch.autograd.inference_mode():\n",
    "        image_features = model.encode_image(image_input)    # CLIP image-based inference\n",
    "        text_features = model.encode_text(text_inputs)      # CLIP text-based inference \n",
    "\n",
    "    # Pick the top 5 most similar labels for the image\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True) # <class 'torch.Tensor'> size [1,512]\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)   # <class 'torch.Tensor'> size [92,512]\n",
    "    #try:\n",
    "    #    print(type(image_features), image_features.shape)\n",
    "    #    print(type(text_features), text_features.shape)\n",
    "    #except:\n",
    "    #    pass\n",
    "    \n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(top_labels)\n",
    "\n",
    "    # Print classification results\n",
    "    print(f\"\\n \\u27A2 {f_jpg.split('/')[-1]}\")\n",
    "    print(f\"   time: {round(time.time() - starttime,2)}s\")\n",
    "\n",
    "    classify_dict = dict()\n",
    "    for i, (value, index) in enumerate(zip(values, indices)):\n",
    "        print(f\"{classids[index]:>20s}: {100 * value.item():.2f}%\")\n",
    "        draw.text((10+width/20, 40*multiplier*(i+1)+height/20),\n",
    "                  classids[index] + \": \" + str(round(100 * value.item(),2)) + \"%\",\n",
    "                  #fill=contrasted_rgb_color,\n",
    "                  #fill=(0  ,176, 24, 20),          # green, 78% opacity\n",
    "                  #fill=(255,236,0  ,255),          # yellow, 100% opacity\n",
    "                  #fill=(255,255,255,255),          # white, 100% opacity\n",
    "                  fill=(255, 40,  0,255),          # red, 100% opacity\n",
    "                  font=label_font2)\n",
    "        classify_dict.update({classids[index]:round(value.item(),4)})\n",
    "        \n",
    "    # Save CLIP classification result in in Numpy nd.array\n",
    "    if str(f) not in img_in_file:                  # image never processed before\n",
    "        ndarr = np.append(ndarr,[[str(f), timestamp, dataset, classify_dict, img_labels]],axis=0)\n",
    "        cnt_new += 1\n",
    "    else:                                          # image processed previously\n",
    "        ndarr[img_in_file.index(str(f))] = [str(f),timestamp,dataset,classify_dict, img_labels]  # 'ValueError' if not in list\n",
    "        \n",
    "    #img.save(res_dir + 'images/' + f + '_cliplabel.jpg') \n",
    "    img.show()        \n",
    "\n",
    "# Transform CLIP classification result from Numpy nd.array to Pandas dataframe and sort by image basename\n",
    "df = DF(ndarr)    \n",
    "df.sort_values(by=[df.columns[0]], ascending=True, inplace=True)    \n",
    "\n",
    "# Commit dataframe to disk\n",
    "df.to_csv(res_dir + 'clip_classify.tsv', mode='w', index=False, sep='\\t', header=header, quoting=csv.QUOTE_ALL)\n",
    "cnt_new = \"No\" if cnt_new == 0 else cnt_new\n",
    "grammar = 'new file was' if cnt_new in {\"No\",1} else 'new files were'\n",
    "print(f'\\n==========================================\\n{cnt_new} {grammar} label-based classified by CLIP and added to:\\n {res_dir + \"clip_classify.tsv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2dceca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python3.7.1 (clip1.0)",
   "language": "python",
   "name": "clip1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
