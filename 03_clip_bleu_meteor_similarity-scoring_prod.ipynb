{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f84ad46",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "Notebook ***\"03_clip_bleu_meteor_similarity-scoring_prod.ipynb\"***:\n",
    "\n",
    "- extracts visual relationships as caption seeds from VISREL analysis results for selected images, i.e. either manually annotated images (training dataset images) or never-seen-before images (test dataset images) processed by an object detection R-CNN model,\n",
    "- post-processes caption-seeds so they conform to the CLIP textual input requirements,\n",
    "- computes image-summary embeddings similarities between reference and candidate summaries.\n",
    "- computes BLEU and METEOR n-gram based similarities between reference and candidate summaries.\n",
    "- tabulates thus obtained results to ease the visual comparison between the scoring schemes.\n",
    "\n",
    "CLIP is the *Contrastive Language-Image Pre-Training* neural network trained on a variety of (image, text) pairs, first reported by Radford et al (2020) and applied by Hessel et al (2021). <BR>\n",
    "\n",
    "CLIP authors claim that given a digitized image, CLIP can produce a descriptive summary of the image, withouthaving specifically fine-tuned for the task, similar to the zero-shot capabilities of OpenAI's Generative Pre-trained Transformer models, GPT-2 and 3.\n",
    "    \n",
    "#### Sequence of actions:\n",
    "**A)** Scan specific directory or zipped archive, where '<basename>.{jpg,png}' files are located<BR>\n",
    "      Run existence checks on files '<basename>.{.img,*.xml,_*_vrd.json}'\n",
    "    \n",
    "**B)** Recover caption seeds obtained with .../vis-rel/07_bbx_visrel_rules.ipynb<BR>\n",
    "      Caption seeds are saved as '<basename>_vrd.json' \n",
    "\n",
    "**C)** Implement CLIP to compute and save cosine similarities between images and caption seeds<BR>\n",
    "      Save result in '../Sgoab/Data/score.tsv'\n",
    "    \n",
    "**D)** Implement ROUGE, BLEU, METEOR n-gram based similarity scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa19108",
   "metadata": {},
   "source": [
    "### Licensing terms and copyright\n",
    "\n",
    "Sections A and B of this notebook are concerned with recovering/loading image captions (e.g. caption-seeds obtained from the VISREL code). The code used to produce caption seeds can be found [here](https://github.com/Cbhihe/VisRel_caption-seeds). It is protected by the terms and conditions of the GNU_GPL-v3 copyleft license.\n",
    "\n",
    "     Copyright (C) 2021 Cedric Bhihe\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under\n",
    "the terms of the GNU General Public License as published by the Free Software\n",
    "Foundation, either version 3 of the License, or any later version.\n",
    "\n",
    "In short, THIS PROGRAM IS MADE AVAILABLE OR DISTRIBUTED IN THE HOPE OF IT BEING USEFUL, \n",
    "BUT WITHOUT WARRANTY OF ANY KIND, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n",
    "MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE.  Look at the GNU General Public \n",
    "License for more details on its terms.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with this program.\n",
    "If not, see <https://www.gnu.org/licenses/>, or consult the License.md file in this repo.\n",
    "\n",
    "=========================================================\n",
    "\n",
    "Section C of this notebook is concerned with the implementation \n",
    "of CLIP, as originally made available by its authors. It is protected by the terms and \n",
    "conditions of the MIT license.\n",
    "\n",
    "    Copyright (C) 2021 OpenAI\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this \n",
    "software and associated documentation files (the \"Software\"), to deal in the Software \n",
    "without restriction, including without limitation the rights to use, copy, modify, merge, \n",
    "publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons \n",
    "to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or \n",
    "substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n",
    "BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, \n",
    "DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "=========================================================\n",
    "\n",
    "***To contact the repo owner for any issue, please open an new thread under the [Issues] tab on this repo.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc09a5",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "This iPython notebook must be executed in a Python virtual environment, running Python v3.7.1. This is a prerequisite so the proper versions of Torch 1.7.1+cpu and TorchVision 0.8.2+cpu can be invoked to run the CLIP 1.0 inference engine on test images. A description of installation steps is detailed in notebook '.../Sgoab/Src/Caption_evaluate/01_clip_cpu_classify.ipynb'.\n",
    "\n",
    "     \n",
    "### Package requirements\n",
    "\n",
    "- Install all required packages in the virtual environment directory \"/path/to/my_directory\", with:\n",
    "```\n",
    "    $ cd /path/to/my_directory\n",
    "    $ python -m pip freeze <<- 'EOF'\n",
    "                clip @ git+https://github.com/openai/CLIP.git@04f4dc2ca1ed0acc9893bd1a3b526a7e02c4bb10ftfy\n",
    "                contractions==0.0.58\n",
    "                Cython==0.29.1\n",
    "                h5py==2.9.0\n",
    "                ftfy==5.5.1\n",
    "                matplotlib==3.0.2\n",
    "                nltk==3.4\n",
    "                numpy==1.17.3\n",
    "                Pillow==8.3.2\n",
    "                pywsd==1.2.4\n",
    "                pyyaml==5.1\n",
    "                regex==2021.8.3\n",
    "                requests==2.20.1\n",
    "                scipy==1.2.0\n",
    "                torch==1.7.1+cpu\n",
    "                torchaudio==0.7.2\n",
    "                torchvision==0.8.2+cpu\n",
    "                tqdm==4.38.0\n",
    "                typing==3.7.4\n",
    "                zipfile37==0.1.3\n",
    "    EOF\n",
    "```\n",
    "From within Python, also do:\n",
    "```\n",
    "        nltk.donwload('punkt',                         # required for pos tagging\n",
    "                      'stopwords',\n",
    "                      'wordnet',                       # required for synsets lookup\n",
    "                      'wordnet_ic',\n",
    "                      'averaged_perceptron_tagger',    # required for pos tagging\n",
    "                      'treebank',\n",
    "                      'names',)\n",
    "    # or instead:\n",
    "        nltk.donwload('popular')\n",
    "```    \n",
    "\n",
    "Some specific resources becomes accessible with the above requirements:\n",
    "\n",
    "    - n-gram based scoring algorithms:\n",
    "        nltk.translate.meteor\n",
    "        nltk.translate.meteor_score\n",
    "        nltk.translate.bleu_score\n",
    "        nltk.corpus.wordnet\n",
    "        nltk.stem.PorterStemmer\n",
    "        nltk.stem.WordNetLemmatizer\n",
    "        nltk.stem.api.StemmerI\n",
    "     \n",
    "    - Word Sense Disambiguation (WSD):   (use `python -m pip install pywsd==1.0.2`)\n",
    "        pywsd.lesk.simple_lesk\n",
    "        pywsd.disambiguate\n",
    "        pywsd.similarity.max_similarity\n",
    "        \n",
    "Jupyter environment requirements include:\n",
    "```\n",
    "                ipykernel==6.6.0\n",
    "                ipython==7.30.1\n",
    "                ipython_genutils==0.2.0\n",
    "                ipywidgets==7.6.5\n",
    "                jupyter_client==7.1.0\n",
    "                jupyter_core==4.9.1\n",
    "                nbclient==0.5.9\n",
    "                nbconvert==6.3.0\n",
    "                nbformat==5.1.3\n",
    "                notebook==5.7.4\n",
    "                traitlets==5.1.1\n",
    "```\n",
    " ... and starting the jupyter notebook from the sytem's jupyter's instance with:\n",
    "```\n",
    "    $ /usr/bin/jupyter notebook 01_clip_cpu_classify\n",
    "```\n",
    "#### Known issues¶\n",
    "\n",
    "- Launching the notebook by relying on the local environment's shims, with:\n",
    "'''\n",
    "$ jupyter notebook 01_clip_cpu_classify\n",
    "'''\n",
    "may fail under Pyenv with a \"Segmentation fault\". It is likely an iPython issue related to jupyter. To avoid it, launch either notebook from a more recent python version, and select iyour custom built 3.7.1 iPython kernel from the notebook at first launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, csv, json, ast, pickle, re, string, random, faulthandler  # builtins\n",
    "from typing import Union, Any, List, Tuple, Optional, Iterable, cast\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "faulthandler.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f82721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.translate import meteor, meteor_score, bleu_score\n",
    "#from nltk.corpus import stopwords, WordNetCorpusReader, wordnet\n",
    "from nltk.corpus import WordNetCorpusReader, wordnet, stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.stem.api import StemmerI\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# partly maintained; install PyPI.org package as: `python -m pip install pywsd==1.0.2`\n",
    "from pywsd import disambiguate      \n",
    "from pywsd.similarity import max_similarity as maxsim\n",
    "from pywsd.lesk import simple_lesk \n",
    "from pywsd.lesk import simple_signature  # read pre-computed signatures per synset\n",
    "                                         # method name changed to 'cached_signature'\n",
    "                                         # in later versions\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "from PIL import Image\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK set-up\n",
    "nltk.download('punkt',                              # required for pos tagging\n",
    "              'averaged_perceptron_tagger',         # required for pos tagging\n",
    "              'stopwords',\n",
    "              'wordnet',                            # required for synsets lookup\n",
    "              'wordnet_ic',                         # required for calculating information content similarity\n",
    "              'treebank',\n",
    "              'names')\n",
    "stop_words = set(stopwords.words('english'))        # English stop-words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ffb36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import trained CLIP model\n",
    "localdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=localdevice, jit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd685f",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text: str, lower=True) -> str:\n",
    "    '''\n",
    "    Remove spaces at begin and end of sole string argument.\n",
    "    Make lower case by default or not if so specified\n",
    "    \n",
    "    @type            text: str\n",
    "    @param           text: input string at end of which any number of spaces are removed\n",
    "    \n",
    "    @return: output string\n",
    "    '''\n",
    "    if lower:\n",
    "       text = text.lower()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def add_end_period(text: str) -> str:\n",
    "    '''\n",
    "    Add period at end of string provided no punctuation is there already.\n",
    "        \n",
    "    @type            text: str\n",
    "    @param           text: input string to end with dot\n",
    "    \n",
    "    @return: output string\n",
    "    '''\n",
    "    text = re.sub(r'([^\\.!?:;,]+$)','\\\\1.',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def download_url(url: Tuple[str,str], my_proxy={ 'http': 'http://127.0.0.2:9151' }) -> None:\n",
    "    \"\"\"\n",
    "    Download function for very large images using chunked stream.\n",
    "     - downloads object located at 'url' in a single process.\n",
    "     - uses default http proxy\n",
    "     - overridden/suppressed). \n",
    "     - default http proxy can allows you to use your system default setting to connect to the Internet.\n",
    "    Use a streaming connection to download larges files in chunks of predetermined sizes.\n",
    "    \n",
    "     @type       url: tuple \n",
    "     @type  my_proxy: dict or None \n",
    "     @param      url: destination_filename, source_filename); both filenames are fully qualified. \n",
    "     @param my_proxy: optional kw arg; default value specific for the system implementation.\n",
    "                                       Replace default dict value with 'None' if needed.\n",
    "                                       \n",
    "     @return: None                                  \n",
    "    \"\"\"\n",
    "    path, url = url\n",
    "    response = req.get(url, stream = True, allow_redirects=True, proxies=my_proxy)\n",
    "    with open(path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size = 2048):\n",
    "            if chunk:\n",
    "                file.write(chunk) \n",
    "\n",
    "                \n",
    "def shorten_string(text: str, context_length: int =64) -> str:\n",
    "    '''\n",
    "    - Develop English contracted verbal forms; e.g. \"it's\" becomes \"it is\". \n",
    "      Contracted possessive forms are kept.\n",
    "    - Shorten provided string to 74 (default) or fewer tokens, counting actual words, \n",
    "      punctuation symbols and two place-holders for start-of-string and end-of-string.\n",
    "    \n",
    "    @type            text: str\n",
    "    @type  context_length: int\n",
    "    @param           text: input string to develop and shorten\n",
    "    @param context_length: maximum length allowed for output string\n",
    "    \n",
    "    @return: output string\n",
    "    '''\n",
    "    text = contractions.fix(text)\n",
    "    if len(word_tokenize(text)) >= context_length:\n",
    "        text = ' '.join(word_tokenize(text)[0:min(context_length-2,64)])\n",
    "        text = re.sub(r' (\\.|!|,|;)','\\\\1',text)\n",
    "        if not re.match(r'.+\\.$',text):\n",
    "             text += '.'\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_visrel_seeds(text: str) -> str:\n",
    "    '''\n",
    "    - Clean provided string following regex pattern substitution.\n",
    "    \n",
    "    @type            text: str\n",
    "    @param           text: input string to clean\n",
    "    \n",
    "    @return: output string\n",
    "    '''\n",
    "    pattern1 = r'\\(\\(|\\)\\)'\n",
    "    pattern2 = r'(\\/([a-z_])*)+'\n",
    "    pattern3 = r'_[0-9]+'\n",
    "    pattern4 = r'_'\n",
    "    pattern5 = r'\\s*[^.]+\\sis\\spartial\\.*\\s*'\n",
    "    \n",
    "    text = re.sub(pattern1,'',text)\n",
    "    text = re.sub(pattern2,'',text)\n",
    "    text = re.sub(pattern3,'',text)\n",
    "    text = re.sub(pattern4,' ',text)\n",
    "    text = re.sub(pattern5,'',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def has_wn_exception(token: str, pos='v') -> str:\n",
    "    '''\n",
    "    Return exception as stem per Wordnet dictionary for given POS encoding\n",
    "        \n",
    "    @type       token: str\n",
    "    @type         pos: str\n",
    "    @param      token: input token to look up\n",
    "    @param        pos: POS tagging per wordnet classification: 'n','v' (default),'a','r' \n",
    "    \n",
    "    @return: output either the input string or the exception stem if any is found.\n",
    "    '''\n",
    "    try:\n",
    "        return wordnet._exception_map[pos][token][0]\n",
    "    except KeyError:     # if pos not in {'n','v','a','r'}\n",
    "        return token\n",
    "    \n",
    "    \n",
    "def stage_vocab_dis(summary: Union[str, List[str]], method: str ='hypernym', similarity: str ='res') -> List[str]:\n",
    "    '''\n",
    "    Stage-Vocabulary-Disambiguation:\n",
    "    \n",
    "    Stage the hypernym based on a Wordnet synset-based information Content similarity score \n",
    "    classification of each token present in 'summary'.\n",
    "    \n",
    "    @type     summary: string, or list (tokenized sentence)\n",
    "    @type      method: string\n",
    "    @type  similarity: string\n",
    "    @param    summary: textual summary to process\n",
    "    @param     method: value = 'hyp'; uses contextualized hypernyms (default)\n",
    "                       value = 'lemma'; uses lemmata, morphological reduction and wordnet\n",
    "                                exception dictionaries\n",
    "    @param similarity: abbreviation of similarity method use to score the disambiguated sense\n",
    "                       and to return a synsets. Similarity methods are one of:\n",
    "                               path: 'wup' (Wu and Palmer, 1994), 'lch' (Leacock and Chodorow, 1994 & 1998),\n",
    "                       info content: 'res' (Resnik, 1993 & 1995), 'jcn' (Jiang and Conrath, 1997), 'lin' (Lin, 1998)\n",
    "                       \n",
    "    @return: tokenized sentence in the form 'List[str]'\n",
    "    \n",
    "    Disregard tokens not composed exclusively of alphabetical characters and stop-words (per NLTK)\n",
    "    and return them unchanged.\n",
    "    \n",
    "    In case of multiple choice among available synsets, disambiguation is performed based on maximizing \n",
    "    similarity between Wordnet definition and context s provided by 'summary'. Once done disambiguation\n",
    "    permits hypernym determination, no matter what the synset tag happens to be. \n",
    "    \n",
    "    To ensure proper pick of noun or verb lemma, use NLTK pos tagging with following classification\n",
    "    labels:\n",
    "        CC Coordinating Conjunction\n",
    "        CD Cardinal Digit\n",
    "        DT Determiner\n",
    "        EX Existential There. Example: “there is” … think of it like “there exists”)\n",
    "        FW Foreign Word.\n",
    "        IN Preposition/Subordinating Conjunction.\n",
    "        JJ Adjective.\n",
    "        JJR Adjective, Comparative.\n",
    "        JJS Adjective, Superlative.\n",
    "        LS List Marker 1.\n",
    "        MD Modal.\n",
    "        NN Noun, Singular.\n",
    "        NNS Noun Plural.\n",
    "        NNP Proper Noun, Singular.\n",
    "        NNPS Proper Noun, Plural.\n",
    "        PDT Predeterminer.\n",
    "        POS Possessive Ending. Example: parent’s\n",
    "        PRP Personal Pronoun. Examples: I, he, she\n",
    "        PRP$ Possessive Pronoun. Examples: my, his, hers\n",
    "        RB Adverb. Examples: very, silently,\n",
    "        RBR Adverb, Comparative. Example: better\n",
    "        RBS Adverb, Superlative. Example: best\n",
    "        RP Particle. Example: give up\n",
    "        TO to. Example: go ‘to’ the store.\n",
    "        UH Interjection. Example: errrrrrrrm\n",
    "        VB Verb, Base Form. Example: take\n",
    "        VBD Verb, Past Tense. Example: took\n",
    "        VBG Verb, Gerund/Present Participle. Example: taking\n",
    "        VBN Verb, Past Participle. Example: taken\n",
    "        VBP Verb, Sing Present, non-3d take\n",
    "        VBZ Verb, 3rd person sing. present takes\n",
    "        WDT wh-determiner. Example: which\n",
    "        WP wh-pronoun. Example: who, what\n",
    "        WP$ possessive wh-pronoun. Example: whose\n",
    "        WRB wh-abverb. Example: where, when\n",
    "        \n",
    "    Alternative methods:\n",
    "        - get all wn grandchildren for a hypernym grandparent ?\n",
    "          https://stackoverflow.com/questions/24217776\n",
    "        - ...\n",
    "    '''\n",
    "    if isinstance(summary,list):\n",
    "        summary_tokenized = [s.lower() for s in summary]\n",
    "        summary = ' '.join(summary_tokenized)\n",
    "        summary = re.sub(r' (\\.|!|,|;|:|\\?)','\\\\1',summary)  # suppress blank before punctuation char: .!,;:?\n",
    "    elif isinstance(summary,str):\n",
    "        summary = summary.lower()\n",
    "        summary_tokenized = word_tokenize(summary)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if method not in {'hypernym','h','hyp','hype','hyper','lemma','l','lem','lemm'}:\n",
    "        method = 'hyp'     # default\n",
    "        \n",
    "    if similarity not in {'wup','lch','res','jcn','lin'}:\n",
    "        similarity = 'res' # default\n",
    "    \n",
    "    summary_dis_lst = list()\n",
    "    \n",
    "    summary_wsd = disambiguate(summary, algorithm=maxsim, similarity_option=similarity, keepLemmas=True)\n",
    "    \n",
    "    if method in {'hypernym','h','hyp','hype','hyper'} :\n",
    "        for _, lmtzd, dis_tok_syn in summary_wsd:\n",
    "            if dis_tok_syn is None or len(dis_tok_syn.hypernyms()) == 0:\n",
    "                hypernym = lmtzd\n",
    "            else:\n",
    "                try:\n",
    "                    hypernym = dis_tok_syn.hypernyms()[0].name().split('.')[0]\n",
    "                except AttributeError:\n",
    "                    hypernym = lmtzd\n",
    "            \n",
    "            summary_dis_lst.append(hypernym)\n",
    "        \n",
    "    elif method in {'lemma','l','lem','lemm'}:\n",
    "        tagging = pos_tag(summary_tokenized)\n",
    "        tok_stem = list()\n",
    "        \n",
    "        for idx, token in enumerate(summary_tokenized):\n",
    "            not_punctuation = True if re.match(r'^[a-zA-Z]+$',token) else False\n",
    "            not_stop_word = True if token not in stop_words else False\n",
    "            len_synsets = len(wordnet.synsets(token))\n",
    "\n",
    "            if (not_punctuation and not_stop_word and len_synsets > 0):\n",
    "\n",
    "                synset_postag = tagging[idx][1]    \n",
    "                if synset_postag.startswith('V'):\n",
    "                    synset_postag = wordnet.VERB      # pos('v')\n",
    "                elif synset_postag.startswith('J'):\n",
    "                    synset_postag = wordnet.ADJ       # pos('a')\n",
    "                elif synset_postag.startswith('R'):\n",
    "                    synset_postag = wordnet.ADV       # pos('r')\n",
    "                else:\n",
    "                    # if synset_postag starts with 'N' or anything not in {'V','J','R'}\n",
    "                    synset_postag = wordnet.NOUN      # pos('n')\n",
    "\n",
    "         \n",
    "                cand_synset_lst = [wordnet.synsets(token)[k] for k in range(len_synsets) \n",
    "                                   if (wordnet.synsets(token)[k].pos() == synset_postag \n",
    "                                       if synset_postag != 'a' \n",
    "                                       else wordnet.synsets(token)[k].pos() in {'a','s'}) ]\n",
    "                \n",
    "                try:\n",
    "                    if summary_wsd[idx][2] in cand_synset_lst:\n",
    "                        try:\n",
    "                            token_lemmata = summary_wsd[idx][2].lemmas()\n",
    "                            token_lemma = token_lemmata[0].name()\n",
    "                        except (AtrributeError, IndexError):\n",
    "                            token_lemma = token # summary_wsd[idx][1]\n",
    "                    else:\n",
    "                        token_lemma = summary_wsd[idx][1]\n",
    "                        \n",
    "                except AttributeError:\n",
    "                    token_lemma = summary_wsd[idx][1]\n",
    "                    \n",
    "                if token_lemma == token:\n",
    "                    try:\n",
    "                        if synset_postag != 'a':\n",
    "                            stem = wordnet._morphy(token, pos=synset_postag)[0]\n",
    "                        else:\n",
    "                            tok_stem.extend(wordnet._morphy(token, pos='a'))\n",
    "                            tok_stem.extend(wordnet._morphy(token, pos='s'))\n",
    "                            tok_stem = list(set(tok_stem))\n",
    "                            stem = tok_stem[0] if len(tok_stem) == 1 else sorted(tok_stem, key=lambda x: len(x), reverse=False)[0]\n",
    "                    except IndexError:\n",
    "                        stem = token\n",
    "                        \n",
    "                    if stem == token_lemma:\n",
    "                        token_lemma = has_wn_exception(stem, pos=synset_postag)\n",
    "            else:\n",
    "                token_lemma = token\n",
    "            \n",
    "            summary_dis_lst.append(token_lemma)\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    summary_dis_tokenized = word_tokenize(' '.join(summary_dis_lst))\n",
    "    \n",
    "    return summary_dis_tokenized\n",
    "\n",
    "\n",
    "def split_on_the_dot(annots_refs_in: List[str]) -> List[List[str]]:\n",
    "    '''\n",
    "    Split provided list of summaries (as strings) on the end-of-sentence period, \n",
    "    whenever a summary is made of more than one sentence.\n",
    "    \n",
    "    @type     annots_refs_in: list of strings\n",
    "    @param    annots_refs_in: textual summaries to be processed\n",
    "\n",
    "    @return:  list of lists of strings; summaries' component sentences as individual \n",
    "              component summaries, ordered as input list was.\n",
    "    '''\n",
    "    annots_refs_split = list()\n",
    "    \n",
    "    annots_refs_in = [re.sub(r'\\.$','',x) for x in annots_refs_in] \n",
    "    annots_refs_in = [s.split('.') for s in annots_refs_in]\n",
    "\n",
    "    for subs in annots_refs_in:\n",
    "        subset = list()\n",
    "        for s in subs:\n",
    "            s += '.'\n",
    "            s = re.sub(r'^\\s+','',s)\n",
    "            subset.append(s)\n",
    "        annots_refs_split.append(subset)\n",
    "    \n",
    "    del annots_refs_in, subs, subset\n",
    "    \n",
    "    return annots_refs_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_scores(idx: int) -> None:\n",
    "    '''\n",
    "    benchmark_scores()::\n",
    "      - computes:\n",
    "          - CLIP img-summary cosine similarities for human references and candidate seeds \n",
    "          ('test' RCNN+VR and 'train' VisRel)\n",
    "        \n",
    "      - computes for both whole and sentence-split summaries:\n",
    "          - METEOR scores between machine candidates and human references \n",
    "          with  stemming and synonymy or lemmatization and synonymy word matches\n",
    "          - BLEU scores between machine candidates and human references\n",
    "          with exact or lemmatized  word matches\n",
    "          \n",
    "        Input:\n",
    "          @param    img: CLIP-preprocessed img\n",
    "          @type     img: pyTorch tensor\n",
    "          \n",
    "        Output:\n",
    "          none\n",
    "    '''\n",
    "    \n",
    "    img = C_imgs_preproc[idx]\n",
    "    file = scored_files[idx]\n",
    "    \n",
    "    img_cands = list()\n",
    "    img_cands_split = list()\n",
    "    \n",
    "    if (file in checklist and\n",
    "        file + '_test_vrd.json_copy' in zipped.namelist() and\n",
    "        file + '_train_vrd.json_copy' in zipped.namelist()\n",
    "       ):\n",
    "        \n",
    "        start = time.time()\n",
    "        #print(f'idx: {idx} and file: {file}')\n",
    "        \n",
    "        # ================\n",
    "        ## CLIP cosine similarities for WHOLE and SPLIT human references\n",
    "        # ================\n",
    "        # authors' list for whole refs\n",
    "        img_refs_authors = [author for author, _ in C_wrk_annots_refs[idx]]\n",
    "        # authors' list for split refs\n",
    "        img_refs_split_authors = [author for author,split_nchunk,_ in C_wrk_annots_refs_split[idx] \n",
    "                                  for _ in range(split_nchunk)]\n",
    "\n",
    "        # whole human ref summaries\n",
    "        img_annots_refs_flat = [shorten_string(annot, context_length=64) for _, annot in C_wrk_annots_refs[idx]]\n",
    "        #print(f'idx: {idx}\\nimg_annots_refs_flat: {img_annots_refs_flat}\\n')\n",
    "        \n",
    "        # split human ref summaries  ('ref_split_summaries'), keeps authors ordering\n",
    "        img_annots_refs_split_flat = [shorten_string(split_annot, context_length=64) \n",
    "                               for _,_,split_txt in C_wrk_annots_refs_split[idx] \n",
    "                               for split_annot in split_txt]\n",
    "        \n",
    "        # list of lists of nltk-tokenized for WHOLE and SPLIT references for given img (at index 'idx')\n",
    "        img_annots_refs_tok = [word_tokenize(ref) for ref in img_annots_refs_flat]\n",
    "        C_annots_refs_tok.append(img_annots_refs_tok)\n",
    "        \n",
    "        img_annots_refs_split_tok = [word_tokenize(ref) for ref in img_annots_refs_split_flat] # flat lst, keeps author order\n",
    "        C_annots_refs_split_tok.append(img_annots_refs_split_tok)   # ordered first by img idx, then by author \n",
    "        \n",
    "        image_input = torch.tensor(np.stack([img,])).to(localdevice)\n",
    "        # build flat list of CLIP tokens for WHOLE human ref summaries for image at idx\n",
    "        img_annots_refs_ctok = clip.tokenize(img_annots_refs_flat).to(localdevice)\n",
    "        # build flat list of CLIP tokens for SPLIT human ref summaries for image at idx   \n",
    "        img_annots_refs_split_ctok = clip.tokenize(img_annots_refs_split_flat).to(localdevice)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_features = model.encode_image(image_input).float().to(localdevice)\n",
    "            img_annots_refs_features = model.encode_text(img_annots_refs_ctok).float().to(localdevice)\n",
    "            img_annots_refs_split_features = model.encode_text(img_annots_refs_split_ctok).float().to(localdevice)\n",
    "\n",
    "        img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "        img_annots_refs_features /= img_annots_refs_features.norm(dim=-1, keepdim=True)\n",
    "        img_annots_refs_split_features /= img_annots_refs_split_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # WHOLE SUMMARIES\n",
    "        S_clip_img_refs = img_annots_refs_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        \n",
    "        # SPLIT SUMMARIES\n",
    "        S_clip_img_refs_split = img_annots_refs_split_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        \n",
    "        S_clip_img_annots_refs_flat = np.ndarray.flatten(S_clip_img_refs).tolist()\n",
    "        S_clip_img_annots_refs_split_flat = np.ndarray.flatten(S_clip_img_refs_split).tolist()\n",
    "        \n",
    "        S_clip_annots_refs_flat.append(S_clip_img_annots_refs_flat)\n",
    "        S_clip_annots_refs_split_flat.append(S_clip_img_annots_refs_split_flat)\n",
    "        \n",
    "        img_refs_best_simil_idx = S_clip_img_annots_refs_flat.index(max(S_clip_img_annots_refs_flat))   # best human scorer according to CLIP\n",
    "        img_refs_split_best_simil_idx = S_clip_img_annots_refs_split_flat.index(max(S_clip_img_annots_refs_split_flat)) # best human scorer with CLIP        \n",
    "        \n",
    "        # ================\n",
    "        ## CLIP cosine similarities for candidate seeds ('test' RCNN+VR and 'train' VisRel)\n",
    "        # ================\n",
    "        \n",
    "        ## load candidates 'test' (RCNN+VR) and 'train' (VR)\n",
    "        vrd_json_files = [f for f in zipped.namelist()\n",
    "                          if (f.endswith('_vrd.json_copy') and f.startswith(file))\n",
    "                         ]\n",
    "        vrd_json_files.sort()\n",
    "        \n",
    "        for f in vrd_json_files: \n",
    "            # load 'test' candidate summary on 1st for-loop pass;\n",
    "            # load 'train' candidate summary on 2nd pass\n",
    "            with zipped.open(f, mode='r') as infile:\n",
    "                data = infile.read()\n",
    "            \n",
    "            data= data.decode('utf8', 'strict')\n",
    "            data_dict = ast.literal_eval(data)\n",
    "            annot_dict = data_dict[\"annot_txt\"]\n",
    "            img_cand_split = [shorten_string(clean_visrel_seeds(seed), context_length=64) \n",
    "                              for key in annot_dict \n",
    "                              for seed in annot_dict[key]] #, sep=',', end=','\n",
    "            \n",
    "            if len(img_cand_split) != 0:\n",
    "                img_cand_split = [seed for seed in set(img_cand_split) if seed != '']\n",
    "            else:\n",
    "                img_cand_split = ['.']\n",
    "            \n",
    "            # list of whole cand summaries ('test','train')\n",
    "            img_cands.append(shorten_string('. '.join(img_cand_split)+'.', context_length=64))\n",
    "            # list of 2 lists of split cand summary seeds ('test','train'), for one image\n",
    "            if len(img_cand_split) != 0:\n",
    "                img_cands_split.append([add_end_period(seed) for seed in img_cand_split if seed not in {'',}])\n",
    "            else:\n",
    "                img_cands_split.append(['.'])\n",
    "        \n",
    "        # C_cands.append(img_cands) <- [['test' cand, 'train' cand],['test' cand, 'train' cand],...], 1 sublist/image\n",
    "        C_cands.append(img_cands)     \n",
    "        C_cands_test.append(img_cands[0])\n",
    "        C_cands_train.append(img_cands[1])\n",
    "        \n",
    "        C_cands_split.append(img_cands_split)\n",
    "        C_cands_split_test.append(img_cands_split[0])\n",
    "        C_cands_split_train.append(img_cands_split[1])\n",
    "        \n",
    "        \n",
    "        img_cands_ntok = list(map(lambda x: len(word_tokenize(x)),img_cands))\n",
    "        C_cands_ntok.append(img_cands_ntok)\n",
    "        \n",
    "        img_cands_split_nchunk = list(map(lambda x: len(x),img_cands_split))\n",
    "        C_cands_split_nchunk.append(img_cands_split_nchunk)\n",
    "        \n",
    "        # structured output: list of 2 lists for 'test' and 'train' nltk-tokenized cands respectively\n",
    "        img_cands_tok = [word_tokenize(seeds) for seeds in img_cands]\n",
    "        # structured output: list made of 2 lists of lists for 'test' split lists and 'train' split lists\n",
    "        #+ respectively; each split list contains a variable nbr of lists of nltk-tokenized split seeds\n",
    "        img_cands_split_tok = [list(map(lambda x: word_tokenize(x),img_cand_split)) \n",
    "                               for img_cand_split in img_cands_split]\n",
    "        \n",
    "        C_cands_tok.append(img_cands_tok)\n",
    "        C_cands_test_tok.append(img_cands_tok[0])\n",
    "        C_cands_train_tok.append(img_cands_tok[1])\n",
    "        \n",
    "        C_cands_split_tok.append(img_cands_split_tok)\n",
    "        C_cands_split_test_tok.append(img_cands_split_tok[0])\n",
    "        C_cands_split_train_tok.append(img_cands_split_tok[1])\n",
    "\n",
    "\n",
    "        # ================\n",
    "        ## compute CLIP cosine similarities for candidate seeds (WHOLE and SPLIT)\n",
    "        # ================\n",
    "        img_cands_ctok = clip.tokenize([seeds for seeds in img_cands]).to(localdevice)\n",
    "        img_cands_split_test_ctok = clip.tokenize(img_cands_split[0]).to(localdevice)\n",
    "        img_cands_split_train_ctok = clip.tokenize(img_cands_split[1]).to(localdevice)\n",
    "                                                                             \n",
    "        with torch.no_grad():\n",
    "            #image_features = model.encode_image(image_input).float().to(localdevice)\n",
    "            img_cands_features = model.encode_text(img_cands_ctok).float().to(localdevice)\n",
    "            img_cands_split_test_features = model.encode_text(img_cands_split_test_ctok).float().to(localdevice)\n",
    "            img_cands_split_train_features = model.encode_text(img_cands_split_train_ctok).float().to(localdevice)\n",
    "        \n",
    "        #image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        img_cands_features /= img_cands_features.norm(dim=-1, keepdim=True)\n",
    "        img_cands_split_test_features /= img_cands_split_test_features.norm(dim=-1, keepdim=True)\n",
    "        img_cands_split_train_features /= img_cands_split_train_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        S_clip_img_cands = img_cands_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        S_clip_img_cands_split_test = img_cands_split_test_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        S_clip_img_cands_split_train = img_cands_split_train_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ================\n",
    "        ## METEOR scores for caption generation with exact/stem/lemmatized/synonym word matches\n",
    "        # ================\n",
    "        # Bib ref.\n",
    "        # A. Lavie and A. Agarwal, “Meteor: an automatic metric for MT evaluation with high\n",
    "        # levels of correlation with human judgments,” in Proc. 2nd Workshop on Statistical\n",
    "        # Machine Translation, Prague, Czech Republic, pp. 228–231 (Jun. 2007).\n",
    "        \n",
    "        \n",
    "        #  ======  METEOR with Porter stemming + synonymy matching\n",
    "        # WHOLE SUMMARIES\n",
    "        S_meteor_img_cands_s = list(map(lambda x: meteor_score.meteor_score(img_annots_refs_tok,x,\n",
    "                                                                             stemmer=PorterStemmer(),\n",
    "                                                                             wordnet=wordnet),\n",
    "                                        img_cands_tok))\n",
    "        \n",
    "        S_meteor_cands_s_test.append(S_meteor_img_cands_s[0])\n",
    "        S_meteor_cands_s_train.append(S_meteor_img_cands_s[1])\n",
    "        \n",
    "        # SPLIT SUMMARIES\n",
    "        S_meteor_img_cands_s_split = [list(map(lambda x: meteor_score.meteor_score(img_annots_refs_split_tok,x,\n",
    "                                                                                   stemmer=PorterStemmer(),\n",
    "                                                                                   wordnet=wordnet),\n",
    "                                               img_cand_split_tok))\n",
    "                                      for img_cand_split_tok in img_cands_split_tok]\n",
    "        \n",
    "        S_meteor_img_cands_s_split_test = S_meteor_img_cands_s_split[0]\n",
    "        S_meteor_img_cands_s_split_train = S_meteor_img_cands_s_split[1]\n",
    "        \n",
    "        S_meteor_cands_s_split_test.append(S_meteor_img_cands_s_split_test)\n",
    "        S_meteor_cands_s_split_train.append(S_meteor_img_cands_s_split_train)\n",
    "        \n",
    "        \n",
    "        # ======  Meteor with lemmatization + synonymy matching\n",
    "        img_annots_refs_l_tok = [stage_vocab_dis(img_ref_tok) for img_ref_tok in img_annots_refs_tok]\n",
    "        img_annots_refs_l_split_tok = [stage_vocab_dis(img_ref_split_tok) for img_ref_split_tok in img_annots_refs_split_tok]\n",
    "        \n",
    "        img_cands_l_tok = [stage_vocab_dis(img_cand_tok) for img_cand_tok in img_cands_tok]\n",
    "        img_cands_l_split_tok = [list(map(lambda x: stage_vocab_dis(x),img_cand_split_tok))\n",
    "                                 for img_cand_split_tok in img_cands_split_tok]\n",
    "        img_cands_l_split_test_tok = img_cands_l_split_tok[:len(img_cands_l_split_tok[0])]\n",
    "        img_cands_l_split_train_tok = img_cands_l_split_tok[len(img_cands_l_split_tok[0]):]\n",
    "        \n",
    "        C_refs_l_tok.append(img_annots_refs_l_tok)        \n",
    "        C_refs_l_split_tok.append(img_annots_refs_l_split_tok)\n",
    "        \n",
    "        C_cands_l_tok.append([img_cands_l_tok[0],img_cands_l_tok[1]])\n",
    "        C_cands_l_test_tok.append(img_cands_l_tok[0])\n",
    "        C_cands_l_train_tok.append(img_cands_l_tok[1])\n",
    "        \n",
    "        C_cands_l_split_tok.append(img_cands_l_split_tok)\n",
    "        C_cands_l_split_test_tok.append(img_cands_l_split_test_tok)\n",
    "        C_cands_l_split_train_tok.append(img_cands_l_split_train_tok)\n",
    "        \n",
    "        # WHOLE SUMMARIES\n",
    "        S_meteor_img_cands_l = list(map(lambda x: meteor_score.meteor_score(img_annots_refs_l_tok,x,\n",
    "                                                                            stemmer=PorterStemmer(),\n",
    "                                                                            wordnet=wordnet),\n",
    "                                        img_cands_l_tok))\n",
    "        \n",
    "        S_meteor_cands_l_test.append(S_meteor_img_cands_l[0])\n",
    "        S_meteor_cands_l_train.append(S_meteor_img_cands_l[1])\n",
    "                \n",
    "        # SPLIT SUMMARIES\n",
    "        S_meteor_img_cands_l_split = [list(map(lambda x: meteor_score.meteor_score(img_annots_refs_l_split_tok,x,\n",
    "                                                                                   stemmer=PorterStemmer(),\n",
    "                                                                                   wordnet=wordnet),\n",
    "\n",
    "                                               img_cand_l_split_tok)) \n",
    "                                      for img_cand_l_split_tok in img_cands_l_split_tok]\n",
    "        \n",
    "        S_meteor_img_cands_l_split_test = S_meteor_img_cands_l_split[0]\n",
    "        S_meteor_img_cands_l_split_train = S_meteor_img_cands_l_split[1]\n",
    "        \n",
    "        S_meteor_cands_l_split_test.append(S_meteor_img_cands_l_split_test)\n",
    "        S_meteor_cands_l_split_train.append(S_meteor_img_cands_l_split_train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ================\n",
    "        ## ======  BLEU (BiLingual Evaluation Understudy)\n",
    "        # ================\n",
    "        # Source code for nltk implementation of BLEU and GLEU scores: \n",
    "        #+      https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "        \n",
    "        # WHOLE SUMMARIES\n",
    "        S_bleu_img_cands_e = list(map(lambda x: bleu_score.sentence_bleu(img_annots_refs_tok,x,\n",
    "                                                                         weights=ngram_weights,\n",
    "                                                                         smoothing_function=bleu_smoothing),\n",
    "                                      img_cands_tok))\n",
    "        \n",
    "        S_bleu_cands_e_test.append(S_bleu_img_cands_e[0])\n",
    "        S_bleu_cands_e_train.append(S_bleu_img_cands_e[1])\n",
    "        \n",
    "        # SPLIT SUMMARIES\n",
    "        S_bleu_img_cands_e_split = [list(map(lambda x: bleu_score.sentence_bleu(img_annots_refs_split_tok,x,\n",
    "                                                                                weights=ngram_weights,\n",
    "                                                                                smoothing_function=bleu_smoothing),\n",
    "                                             img_cand_split_tok))\n",
    "                                    for img_cand_split_tok in img_cands_split_tok]\n",
    "        \n",
    "        S_bleu_cands_e_split_test.append(np.mean(S_bleu_img_cands_e_split[0]))\n",
    "        S_bleu_cands_e_split_train.append(np.mean(S_bleu_img_cands_e_split[1]))\n",
    "        \n",
    "        # WHOLE\n",
    "        S_bleu_img_cands_l = list(map(lambda x: bleu_score.sentence_bleu(img_annots_refs_l_tok,x,\n",
    "                                                                         weights=ngram_weights,\n",
    "                                                                         smoothing_function=bleu_smoothing),\n",
    "                                      img_cands_l_tok))\n",
    "        \n",
    "        S_bleu_cands_l_test.append(S_bleu_img_cands_l[0])\n",
    "        S_bleu_cands_l_train.append(S_bleu_img_cands_l[1])\n",
    "        \n",
    "        # SPLIT\n",
    "        S_bleu_img_cands_l_split = [list(map(lambda x: bleu_score.sentence_bleu(img_annots_refs_l_split_tok,x,\n",
    "                                                                                weights=ngram_weights,\n",
    "                                                                                smoothing_function=bleu_smoothing),\n",
    "                                             img_cand_l_split_tok))\n",
    "                                    for img_cand_l_split_tok in img_cands_l_split_tok]\n",
    "        \n",
    "        S_bleu_cands_l_split_test.append(np.mean(S_bleu_img_cands_l_split[0]))\n",
    "        S_bleu_cands_l_split_train.append(np.mean(S_bleu_img_cands_l_split[1]))  \n",
    "        \n",
    "        \n",
    "        ## Collect CLIP similarity score for each split human ref per image --> simils_img_refs_split\n",
    "        img_wrk_annots_refs_split = C_wrk_annots_refs_split[idx]\n",
    "        for ii, (_,l,_) in enumerate(img_wrk_annots_refs_split):\n",
    "            img_split_start = sum([l for _,l,_ in img_wrk_annots_refs_split[:ii]])\n",
    "            simils_img_refs_split = S_clip_img_annots_refs_split_flat[img_split_start:img_split_start+l]\n",
    "        \n",
    "        ## Note: \n",
    "        # When CPU based multiprocessing is used, if scores appended to file for each image \n",
    "        #+ requires posterior dynamic access, append operations order must be preserved. Affected files are:\n",
    "        #+ S_bleu_cands_e_test\n",
    "        #+ S_bleu_cands_e_train\n",
    "        #+ S_bleu_cands_e_split_test\n",
    "        #+ S_bleu_cands_e_split_train\n",
    "        #+ S_bleu_cands_l_test\n",
    "        #+ S_bleu_cands_l_train\n",
    "        #+ S_bleu_cands_l_split_test\n",
    "        #+ S_bleu_cands_l_split_train\n",
    "        #+ S_meteor_cands_s_test\n",
    "        #+ S_meteor_cands_s_train\n",
    "        #+ S_meteor_cands_s_split_test\n",
    "        #+ S_meteor_cands_s_split_train\n",
    "        #+ S_meteor_cands_l_test\n",
    "        #+ S_meteor_cands_l_train\n",
    "        #+ S_meteor_cands_l_split_test\n",
    "        #+ S_meteor_cands_l_split_train\n",
    "        \n",
    "        # In that context, in order to prevent  possible race conditions a lock or a semaphore mechanism\n",
    "        #+ encompassing all append operations as a unique block should be implementated.\n",
    "        \n",
    "        '''\n",
    "          File      Cand length |          CLIP             |   METEOR_s   |   METEOR_l   |    BLEU_e   |   BLEU_l\n",
    "                         Te/Tr  | HRef_mx  RCNN+VR      VR  | RCNN+VR  VR  | RCNN+VR  VR  | RCNN+VR  VR | RCNN+VR  VR\n",
    "        '''\n",
    "        print(f'\\\n",
    "{file[0:11]:<9s}... w {img_cands_ntok[0]:>3d}/{img_cands_ntok[1]:<3d}\\\n",
    "{round(100*S_clip_img_annots_refs_flat[img_refs_best_simil_idx],2):>9.2f}%\\\n",
    "{round(100*list(S_clip_img_cands[:,0])[0],2):>8.2f}%\\\n",
    "{round(100*list(S_clip_img_cands[:,0])[1],2):>8.2f}%\\\n",
    "{100*S_meteor_img_cands_s[0]:>9.0f}\\\n",
    "{100*S_meteor_img_cands_s[1]:>5.0f}\\\n",
    "{100*S_meteor_img_cands_l[0]:>9.0f}\\\n",
    "{100*S_meteor_img_cands_l[1]:>5.0f}\\\n",
    "{100*S_bleu_img_cands_e[0]:>9.0f}\\\n",
    "{100*S_bleu_img_cands_e[1]:>5.0f}\\\n",
    "{100*S_bleu_img_cands_l[0]:>9.0f}\\\n",
    "{100*S_bleu_img_cands_l[1]:>5.0f}\\n\\\n",
    "WCT:{(time.time()-start)//60:3.0f}:{(time.time()-start)%60:02.0f}{\"s\":>6s}\\\n",
    "{round(100*np.mean(simils_img_refs_split),2):>17.2f}%\\\n",
    "{round(100*np.mean(S_clip_img_cands_split_test[:,0]),2):>8.2f}%\\\n",
    "{round(100*np.mean(S_clip_img_cands_split_train[:,0]),2):>8.2f}%\\\n",
    "{100*np.mean(S_meteor_img_cands_s_split_test):>9.0f}\\\n",
    "{100*np.mean(S_meteor_img_cands_s_split_train):>5.0f}\\\n",
    "{100*np.mean(S_meteor_img_cands_l_split_test):>9.0f}\\\n",
    "{100*np.mean(S_meteor_img_cands_l_split_train):>5.0f}\\\n",
    "{100*np.mean(S_bleu_img_cands_e_split[0]):>9.0f}\\\n",
    "{100*np.mean(S_bleu_img_cands_e_split[1]):>5.0f}\\\n",
    "{100*np.mean(S_bleu_img_cands_l_split[0]):>9.0f}\\\n",
    "{100*np.mean(S_bleu_img_cands_l_split[1]):>5.0f}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2613882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish minimal runtime directory structure\n",
    "data_dir = 'Data_git/'\n",
    "annot_dir = data_dir + 'Annots/'\n",
    "\n",
    "# Load zipped crowdsourced files\n",
    "infile = data_dir + 'crowd_set.zip'\n",
    "with ZipFile(infile,'r') as zipped:\n",
    "    img_lst = ['.'.join(f.split(sep='.')[:-1]) for f in zipped.namelist() if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    img_lst.sort()\n",
    "    xml_lst = ['_train.'.join(f.split(sep='_train.')[:-1]) for f in zipped.namelist() if f.endswith('_train.xml')]\n",
    "    xml_lst.sort()\n",
    "\n",
    "files = [f for f in img_lst if f in xml_lst]\n",
    "\n",
    "# Load crowdsourced annotations\n",
    "with open(annot_dir + 'crowd-ref-annots.tsv', 'r', newline='') as infile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    ncol = len(header)\n",
    "    ndarr = np.empty([0,ncol],dtype='str')\n",
    "    for row in reader:\n",
    "        ndarr = np.append(ndarr,[row],axis=0)\n",
    "        \n",
    "camp_annots_workers = [str(x) for x in list(ndarr[:,1])]\n",
    "camp_annots_imgs = ['.'.join(str(x).split(sep='.')[:-1]) for x in list(ndarr[:,2])]     # img filename w/ infix\n",
    "camp_annots_refs = [shorten_string(cleanup(str(x).replace('  ',' '))) for x in list(ndarr[:,5])]\n",
    "camp_annots_refs_split = split_on_the_dot(camp_annots_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post process R-CNN + VisRel visual relationships as stored in \"*_vrd.json\" files\n",
    "\n",
    "infix = (\"_train\",\"_test\",\"\")\n",
    "\n",
    "checklist = list(dict.fromkeys(camp_annots_imgs))   # eliminate dupes\n",
    "max_to_show = min(24,len(files))\n",
    "prev_state = random.getstate()\n",
    "random.seed()\n",
    "f_to_score = random.sample(files,max_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ac5f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Retrieve human ref summaries, calculate image/summary similarities with CLIP, \n",
    "#+ display results for whole summaries and for summaries split-on-the-dot.\n",
    "\n",
    "scored_files =  list()\n",
    "C_imgs_origin =  list()\n",
    "C_imgs_preproc =  list()\n",
    "\n",
    "txts =  list()       # list of sublists of strings; each sublist contains ordered whole human\n",
    "                     #+  refs per image index-ordered per image, then per author.\n",
    "txts_split =  list() # list of lists of sublist of strings; each sublist contains available human refs,\n",
    "                     #+ split-on-the-dot for image; index-ordered per image, then per author, \n",
    "                     #+ then per sub-sentence\n",
    "C_wrk_annots_refs =  list()\n",
    "C_wrk_annots_refs_split =  list()\n",
    "\n",
    "S_clip_refs_flat = list()\n",
    "S_clip_refs_split_flat = list()\n",
    "\n",
    "plt.figure(figsize=(10*max_to_show, 4*max_to_show))\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "zipped = ZipFile(data_dir + 'crowd_set.zip','r')  # in-memory\n",
    "zipinfo = zipped.infolist()\n",
    "\n",
    "for file in f_to_score:\n",
    "    str_to_plt = ''\n",
    "    str_to_print = ''\n",
    "    \n",
    "    if (file in checklist and file + '.jpg' in zipped.namelist()):\n",
    "        infile = zipped.open(file + '.jpg')\n",
    "        img = Image.open(infile).convert('RGBA')\n",
    "        img_preproc = preprocess(img)\n",
    "        \n",
    "        C_imgs_origin.append(img)\n",
    "        C_imgs_preproc.append(img_preproc)\n",
    "        \n",
    "        scored_files.append(file)\n",
    "        \n",
    "        img_wrk_annots_refs = [(camp_annots_workers[idx],camp_annots_refs[idx]) \n",
    "                               for idx, pix_name in enumerate(camp_annots_imgs) \n",
    "                               if pix_name == file]\n",
    "        img_wrk_annots_refs_split = [(camp_annots_workers[idx],len(camp_annots_refs_split[idx]),camp_annots_refs_split[idx])\n",
    "                                     for idx, pix_name in enumerate(camp_annots_imgs)\n",
    "                                     if pix_name == file]              # Artem's idea\n",
    "        \n",
    "        C_wrk_annots_refs.append(img_wrk_annots_refs)                  # list of (_,_) tuples\n",
    "        C_wrk_annots_refs_split.append(img_wrk_annots_refs_split)      # list of (_,_,_) tuples\n",
    "\n",
    "        \n",
    "        #img_refs_lst\n",
    "        img_refs_flat = [annot for _,annot in img_wrk_annots_refs]\n",
    "        #img_refs_split_lst\n",
    "        img_refs_split_flat = [split_annot \n",
    "                               for _,_,split_annots in img_wrk_annots_refs_split \n",
    "                               for split_annot in split_annots]              # flattened list of split annots for img\n",
    "        \n",
    "        txts.append(img_refs_flat)\n",
    "        txts_split.append([split_annots \n",
    "                           for _,_,split_annots in img_wrk_annots_refs_split])  # structured list of lists of split annots for img batch\n",
    "        \n",
    "        img_input = torch.tensor(np.stack([img_preproc,])).to(localdevice)  # 'img_preproc' can be numpy array(np.uint8) or PIL img\n",
    "        img_refs_ctok = clip.tokenize(img_refs_flat).to(localdevice)\n",
    "        img_refs_split_ctok = clip.tokenize(img_refs_split_flat).to(localdevice)\n",
    "        img_refs_authors = [author for author, _ in img_wrk_annots_refs]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            img_features = model.encode_image(img_input).float().to(localdevice)\n",
    "            img_refs_features = model.encode_text(img_refs_ctok).float().to(localdevice)\n",
    "            img_refs_split_features = model.encode_text(img_refs_split_ctok).float().to(localdevice)\n",
    "        \n",
    "        img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "        img_refs_features /= img_refs_features.norm(dim=-1, keepdim=True)\n",
    "        img_refs_split_features /= img_refs_split_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        #similarity\n",
    "        S_clip_img_refs = img_refs_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        #similarity_split\n",
    "        S_clip_img_refs_split = img_refs_split_features.cpu().numpy() @ img_features.cpu().numpy().T\n",
    "        \n",
    "        #similarities \n",
    "        S_clip_img_refs_flat= np.ndarray.flatten(S_clip_img_refs).tolist()\n",
    "        #similarities_split \n",
    "        S_clip_img_refs_split_flat = np.ndarray.flatten(S_clip_img_refs_split).tolist()\n",
    "        \n",
    "        S_clip_refs_flat.append(S_clip_img_refs_flat)\n",
    "        S_clip_refs_split_flat.append(S_clip_img_refs_split_flat)\n",
    "        \n",
    "        print('\\n'+file, len(img_refs_flat), '('+'+'.join([str(subset[1]) \n",
    "                                                           for subset in img_wrk_annots_refs_split])+')'\n",
    "             )\n",
    "        \n",
    "        split_start = []\n",
    "        similarity_split_to_print = []\n",
    "        \n",
    "        for idx, (_,l,_) in enumerate(img_wrk_annots_refs_split):\n",
    "            split_start = sum([l for _,l,_ in img_wrk_annots_refs_split[:idx]])\n",
    "            similarity_split_to_print = S_clip_img_refs_split[split_start:split_start+l,0].tolist()\n",
    "            average = round(100*np.mean(similarity_split_to_print),1)\n",
    "            str_intermed = ';'.join([str(round(100*x,1))+'%' for x in similarity_split_to_print])\n",
    "            str_to_print = f'{100*S_clip_img_refs[idx,0]:49.2f}% ({str_intermed} Mean:{average}%)'\n",
    "            print(str_to_print)\n",
    "        \n",
    "        for idx, (author, summary) in enumerate(img_wrk_annots_refs):\n",
    "            str_to_plt += f'\\n{author:>8}: {100*S_clip_img_refs[idx,0]:.2f}%: \\\"{shorten_string(summary, context_length=64)}\\\"'\n",
    "        str_to_plt += f'\\nAverage cosine similarity (CLIP v1.0): {100*np.mean(S_clip_img_refs_flat):.1f}%'\n",
    "        \n",
    "        # prepare 14 rows x 2 cols grid, where each img uses 4 grid spots (eg. 1,2,3,4 then 5,6,7,8, etc.)\n",
    "        #+ counting left to right then continuing one grid row down\n",
    "        plt.subplot(2*max_to_show, 2, (4*(len(C_imgs_preproc)-1)+1, 4*(len(C_imgs_preproc)-1)+4))\n",
    "        plt.imshow(img)\n",
    "        plt.text(img.size[0]*1.1, img.size[1], \n",
    "                 file + str_to_plt, \n",
    "                 fontsize=12)\n",
    "        plt.subplots_adjust(right=0.75) #,hspace=0.4)  # make room for text\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(f\"Time to complete {time.time()-start}s\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc2d62a5",
   "metadata": {},
   "source": [
    "%%bash -s \"$img_dir\"\n",
    "infix=(\"_train\" \"_test\")\n",
    "cd $1\n",
    "pwd\n",
    "\n",
    "#ser_infix=\"_train\"\n",
    "#ser_infix=\"_test\"\n",
    "#ser_infix=\"\"\n",
    "\n",
    "for suf in ${infix[@]}; do\n",
    "    for file in *\"$suf\"_vrd.json; do\n",
    "        cp -f $file ${file}_copy\n",
    "        sed -i 's/^\"//;s/\"$//;s/\\\\//g' \"${file}_copy\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8114f6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Generate scores:\n",
    "# ================\n",
    "# - CLIP individual candidates' cosine similarities: human, RCNN+VR, VR \n",
    "# - METEOR: individual candidates' RCNN+VR, VR vs. human refs.\n",
    "# - BLEU: individual candidates' and corpus based RCNN+VR, VR vs. human refs.\n",
    "\n",
    "C_refs_tok = list()\n",
    "C_refs_split_tok = list()\n",
    "\n",
    "C_refs_l_tok = list()\n",
    "C_refs_l_split_tok = list()\n",
    "\n",
    "C_annots_refs_tok = list()\n",
    "C_annots_refs_split_tok = list()\n",
    "\n",
    "C_cands = list()\n",
    "C_cands_test = list()\n",
    "C_cands_train = list()\n",
    "\n",
    "C_cands_split = list()\n",
    "C_cands_split_test = list()\n",
    "C_cands_split_train = list()\n",
    "\n",
    "C_cands_ntok = list()           # nbr of nltk-tokens in whole cands (RCNN+VR 'test', VR'train')\n",
    "C_cands_split_nchunk = list()   # nbr of chunks in split cands list(RCNN+VR 'test', VR'train')\n",
    "\n",
    "C_cands_tok = list()\n",
    "C_cands_test_tok = list()\n",
    "C_cands_train_tok = list()\n",
    "\n",
    "C_cands_split_tok = list()\n",
    "C_cands_split_test_tok = list()\n",
    "C_cands_split_train_tok = list()\n",
    "\n",
    "C_cands_l_tok = list()\n",
    "C_cands_l_test_tok = list()\n",
    "C_cands_l_train_tok = list()\n",
    "\n",
    "C_cands_l_split_tok = list()\n",
    "C_cands_l_split_test_tok = list()\n",
    "C_cands_l_split_train_tok = list()\n",
    "\n",
    "S_clip_refs_flat = list()\n",
    "S_clip_annots_refs_flat = list()\n",
    "S_clip_annots_refs_split_flat = list()\n",
    "\n",
    "S_meteor_cands_s_test = list()          # Meteor w/ Porter stemmer v1\n",
    "S_meteor_cands_s_train = list()         #  id\n",
    "S_meteor_cands_s_split_test = list()    # Meteor w/ split-on-the-dot and Porter stemmer v1 treatment\n",
    "S_meteor_cands_s_split_train = list()   #  id\n",
    "\n",
    "S_meteor_cands_l_test = list()          # Meteor w/ lemmatization (prior to application of Porter stemmer v1) \n",
    "S_meteor_cands_l_train = list()         #  id\n",
    "S_meteor_cands_l_split_test = list()    # Meteor w/ split-on-the-dot, lemmatization (prior to application of Porter stemmer v1) \n",
    "S_meteor_cands_l_split_train = list()   #  id\n",
    "\n",
    "S_bleu_cands_e_test = list()\n",
    "S_bleu_cands_e_train = list()\n",
    "S_bleu_cands_e_split_test = list()\n",
    "S_bleu_cands_e_split_train = list()\n",
    "\n",
    "S_bleu_cands_l_test = list()\n",
    "S_bleu_cands_l_train = list()\n",
    "S_bleu_cands_l_split_test = list()\n",
    "S_bleu_cands_l_split_train = list()\n",
    "\n",
    "ngram_max_n = 4\n",
    "ngram_weights = [0.35, 0.45, 0.1, 0.1]   # default: [1/ngram_max_n for _ in range(ngram_max_n)]\n",
    "bleu_smoothing = bleu_score.SmoothingFunction().method1     # output 0.08, 0.09\n",
    "\n",
    "print(f\"               VR length |          CLIP             |   METEOR_s  |   METEOR_l  |    BLEU_e   |   BLEU_l\")\n",
    "print(f\" File             Te/Tr  | HRef_mx  RCNN+VR     VR   | RCNN+VR  VR | RCNN+VR  VR | RCNN+VR  VR | RCNN+VR  VR\")\n",
    "\n",
    "for idx in range(len(C_imgs_preproc)):\n",
    "    benchmark_scores(idx)\n",
    "\n",
    "print(f'\\n{\"Campaign scores:  BLEU_e  BLEU_l  METEOR_s  METEOR-l\"}\\n\\\n",
    "{\"                 (M-ave) (M-ave)  (M-ave)   (M-ave)\"}\\n\\\n",
    "{\"R-CNN+VR (Te) w\":>16s}:  \\\n",
    "{100*np.mean(S_bleu_cands_e_test):4.0f}  \\\n",
    "{100*np.mean(S_bleu_cands_l_test):5.0f}  \\\n",
    "{100*np.mean(S_meteor_cands_s_test):8.0f}  \\\n",
    "{100*np.mean(S_meteor_cands_l_test):7.0f}\\n\\\n",
    "{\"s\":>15s}:  \\\n",
    "{100*np.mean(S_bleu_cands_e_split_test):4.0f}  \\\n",
    "{100*np.mean(S_bleu_cands_l_split_test):5.0f}  \\\n",
    "{100*np.mean([score for scores in S_meteor_cands_s_split_test for score in scores]):8.0f}  \\\n",
    "{100*np.mean([score for scores in S_meteor_cands_l_split_test for score in scores]):7.0f}\\n\\\n",
    "{\"VR (Tr) w\":>16s}:  \\\n",
    "{100*np.mean(S_bleu_cands_e_train):4.0f}  \\\n",
    "{100*np.mean(S_bleu_cands_l_train):5.0f}  \\\n",
    "{100*np.mean(S_meteor_cands_s_train):8.0f}  \\\n",
    "{100*np.mean(S_meteor_cands_l_train):7.0f}\\n\\\n",
    "{\"s\":>15s}:  \\\n",
    "{100*np.mean(S_bleu_cands_e_split_train):4.0f}  \\\n",
    "{100*np.mean(S_bleu_cands_l_split_train):5.0f}  \\\n",
    "{100*np.mean([score for scores in S_meteor_cands_s_split_train for score in scores]):8.0f}  \\\n",
    "{100*np.mean([score for scores in S_meteor_cands_l_split_train for score in scores]):7.0f}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5efbe425",
   "metadata": {},
   "source": [
    "## Example output for files:\n",
    "['004a409ae-748e-7a16-0a9f-950c815a6a3e', '0057e0bfd-d9a3-352b-c14c-149f42f08287', '006c15d6f-892f-c42d-12df-5595475a85c6', '006fcb058-3833-f11b-5ba9-e05c80dbd0cd', '0009c70ac-99e9-70a8-5989-627d1fb53b3a', '0027eb131-72ca-f42e-36ef-d007b1777a34']\n",
    "# ##################################\n",
    "\n",
    "               VR length |          CLIP             |   METEOR_s   |   METEOR_l   |    BLEU_e   |   BLEU_l\n",
    " File             Te/Tr  | HRef_mx  RCNN+VR     VR   | RCNN+VR  VR  | RCNN+VR  VR  | RCNN+VR  VR | RCNN+VR  VR\n",
    "004a409ae-7... w  12/12     38.19%   21.16%   21.16%       11   11        17   17         3    3        9    9\n",
    "WCT:  5:52     s            27.06%   20.99%   20.99%       31   31        41   41         9    9       14   14\n",
    "0057e0bfd-d... w  30/43     36.24%   27.49%   27.33%       16   19        16   18        18   18       16   16\n",
    "WCT: 18:58     s            23.86%   24.31%   23.38%       48   43        34   37        35   33       26   27\n",
    "006c15d6f-8... w   1/20     34.53%   20.18%   19.80%        0   20         0   20         0    6        0    6\n",
    "WCT: 17:08     s            25.59%   19.85%   19.39%       11   24        11   24         0   11        0   10\n",
    "006fcb058-3... w   8/63     31.62%   17.52%   23.96%        7   22         7   21         1    7        1   10\n",
    "WCT: 18:09     s            23.06%   18.34%   21.95%       26   36        26   37        20   29       19   33\n",
    "0009c70ac-9... w  35/24     31.82%   26.97%   28.13%       19   40        20   40         6   10        6   10\n",
    "WCT:  7:53     s            28.48%   26.14%   25.25%       29   38        28   37        12   18       12   18\n",
    "0027eb131-7... w   1/1      31.21%   21.08%   21.08%        0    0         0    0         0    0        0    0\n",
    "WCT: 14:06     s            23.61%   21.11%   21.11%       11   11        11   11         0    0        0    0\n",
    "\n",
    "Campaign scores:  BLEU_e  BLEU_l  METEOR_s  METEOR-l\n",
    "                 (M-ave) (M-ave)  (M-ave)   (M-ave)\n",
    "R-CNN+VR (Te)  w:     5      5        9       10\n",
    "               s:    13     12       33       30\n",
    "      VR (Tr)  w:     7      9       19       19\n",
    "               s:    17     17       36       36"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7.1 (clip1.0)",
   "language": "python",
   "name": "clip1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
