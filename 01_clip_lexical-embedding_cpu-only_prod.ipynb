{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82452c5",
   "metadata": {},
   "source": [
    "### Problem statementÂ¶\n",
    "\n",
    "This notebook does 1 thing:\n",
    "\n",
    "- Load CLIP\n",
    "- Compute natural language embeddings for tokens of interest using the CLIP transformer model.\n",
    "\n",
    "_CLIP is the *Contrastive Language-Image Pre-Training* neural network trained on a variety of (image, text) pairs, first reported by Radford et al (2020) and applied by Hessel et al (2021). <BR>\n",
    "\n",
    "CLIP authors claim that given a digitized image, it can produce a relevant textual description, without having been specifically fine-tuned for the task, in a way similar to the one-shot capabilities of GPT-2 and 3._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bde0f4",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "This iPython notebook must be run in a Python virtual environment, running Python v3.7.0. This is a prerequisite so the proper versions of Torch 1.7.0+cpu and TorchVision 0.8.1+cpu can be invoked to run the CLIP 1.0 inference engine. Instructions to install CLIP are provided below for Linux hosts, assuming that:\n",
    "\n",
    "- your interactive terminal session executes a Bourne shell (`sh`) or a Bourne derivative ( `ksh`, `bash`, ...).\n",
    "- you already set up a Python 3.7.0 virtual environment, in directory `/path/to/my_directory`.\n",
    "- you know how to handle CLI in terminal.\n",
    "\n",
    "#### Setting up and registering a custom iPython kernel\n",
    "\n",
    "What follows applies to CPU-only constrained installations. For CUDA-enabled machines, refer to `https://github.com/OpenAI/CLIP`.\n",
    "\n",
    "- Assuming you have configured `pyenv` (on your favorite Linux host) to manage multiple Python virtual environments with specific package requirements, choose the directory in which to setup your Python virtual environment and install your iPython kernel utility package `ipykernel`:\n",
    "\n",
    "```\n",
    "      $ cd /path/to/my_directory\n",
    "      $ pyenv local 3.7.0\n",
    "      $ python -m pip install ipykernel\n",
    "```\n",
    "- Install every required packages in the virtual environment directory `/path/to/my_directory` (see following section for that).\n",
    "    \n",
    "```\n",
    "      $ python -m pip install torch==1.7.0+cpu torchvision==0.8.0+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "      $ python -m pip install ftfy regex tqdm\n",
    "      $ python -m  pip install git+https://github.com/openai/CLIP.git\n",
    "```\n",
    "\n",
    "- Make the new custom iPython kernel, clip1.0, available in interactive Python sessions:\n",
    "```\n",
    "      $ cd /path/to/my_directory\n",
    "      $ ipython kernel install --user --name clip1.0 --display-name \"Python3.7.0 (clip1.0)\"\n",
    "      $ jupyter notebook        # launch an iPython session\n",
    "```\n",
    "\n",
    "- Select the special virtual environment kernel ***Python 3.7.0 (clip1.0)*** under the `New notebook` button in the top-right region of the browser page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e5644",
   "metadata": {},
   "source": [
    "### Package requirements\n",
    "\n",
    "Package requirements are detailed below. For a quick demo also install `Pillow==8.2.0` and dependencies.\n",
    "\n",
    "- Install all required packages in the virtual environment directory \"/path/to/my_directory\", with:\n",
    "```\n",
    "    $ cd /path/to/my_directory\n",
    "    $ python -m pip install <<- 'EOF'\n",
    "                clip @ git+https://github.com/openai/CLIP.git@04f4dc2ca1ed0acc9893bd1a3b526a7e02c4bb10ftfy\n",
    "                Cython==0.29.22\n",
    "                ftfy==6.0.3\n",
    "                ipykernel==5.5.3\n",
    "                ipyparallel==6.3.0\n",
    "                ipython==7.22.0\n",
    "                ipython-genutils==0.2.0\n",
    "                ipywidgets==7.6.3\n",
    "                nltk==3.6.5\n",
    "                numpy==1.18.5\n",
    "                regex==2021.10.8\n",
    "                torch==1.7.0+cpu\n",
    "                torchaudio==0.7.0\n",
    "                torchvision==0.8.1+cpu\n",
    "                tqdm==4.61.1\n",
    "                scipy==1.6.2\n",
    "    EOF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c5143",
   "metadata": {},
   "source": [
    "### Licensing terms and copyright\n",
    "\n",
    "A substantial portion of the following code, is based on CLIP, as originally made available by its authors. CLIP, its use and distribution are protected by the terms and conditions of the MIT license.\n",
    "\n",
    "    Copyright (c) 2021 OpenAI\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "=================================\n",
    "\n",
    "The pre-processing and wrapper code used to compute CLIP vector embeddings in this notebook are protected.\n",
    "\n",
    "    Copyright (C) 2020,2021 Cedric Bhihe\n",
    "\n",
    "The OpenAI software made available in this repository was extended with pre- and post-processing steps. The corresponding code is available for free under the terms of the Gnu GPL License version 3 or later.\n",
    "\n",
    "In short, you are welcome to use and distribute the code of this repo as long as you always quote its author and the Licensing terms in full. Consult the licensing terms and conditions in License.md on this repo.\n",
    "\n",
    "=================================\n",
    "\n",
    "***To contact the repo owner for any issue, please open an new thread under the [Issues] tab of the source code repo.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbbf465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, csv, time\n",
    "from datetime import datetime as dt \n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d424d5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "## Check pyTorch version\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238f4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Image input resolution: 224 by 224 pixels\n",
      "Textual context length: 77\n",
      "Total vocabulary size: 49408\n",
      "Local device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Import trained CLIP model\n",
    "localdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=localdevice, jit=False)\n",
    "#model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length   # pixel size of resized square picture side\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Image input resolution:\", input_resolution,\"by\",input_resolution,\"pixels\")\n",
    "print(\"Textual context length:\", context_length)\n",
    "print(\"Total vocabulary size:\", vocab_size)\n",
    "print(\"Local device:\", localdevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298121de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CLIP(\n",
       "   (visual): VisionTransformer(\n",
       "     (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "     (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (transformer): Transformer(\n",
       "       (resblocks): Sequential(\n",
       "         (0): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (1): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (2): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (3): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (4): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (5): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (6): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (7): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (8): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (9): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (10): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (11): ResidualAttentionBlock(\n",
       "           (attn): MultiheadAttention(\n",
       "             (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): Sequential(\n",
       "             (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (gelu): QuickGELU()\n",
       "             (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (transformer): Transformer(\n",
       "     (resblocks): Sequential(\n",
       "       (0): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (1): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (2): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (3): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (4): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (5): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (6): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (7): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (8): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (9): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (10): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (11): ResidualAttentionBlock(\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Sequential(\n",
       "           (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (gelu): QuickGELU()\n",
       "           (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "         )\n",
       "         (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (token_embedding): Embedding(49408, 512)\n",
       "   (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " Compose(\n",
       "     Resize(size=224, interpolation=PIL.Image.BICUBIC)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     <function _convert_image_to_rgb at 0x7f1dbd24dbf8>\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load model of choice (not JIT), and return info on:\n",
    "#   - VisionTransformer (for images) and \n",
    "#   - Transformer (for text)\n",
    "clip.load(\"ViT-B/32\", device=localdevice, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f633a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLIP contextualized embedding for 'cover':\n",
      "========================\n",
      "\n",
      "Summary: Her long sleeve covered the wound completely. Nobody could possibly unmask her cover-up, she thought.\n",
      "    -- time elapsed: 19.832ms\n",
      "Found tokens (position-index,embedding): [(3, 5603), (13, 899)]\n",
      "\n",
      "Summary: \"The proposal ought to have been covering the keypoints in question!\", he said non-plused.\n",
      "    -- time elapsed: 2.242ms\n",
      "Found tokens (position-index,embedding): [(7, 9462)]\n",
      "\n",
      "CLIP contextualized embedding for 'ride':\n",
      "========================\n",
      "\n",
      "Summary: I am a poor lonesome cowbow, riding in the sunset...\n",
      "    -- time elapsed: 0.271ms\n",
      "Found tokens (position-index,embedding): []\n",
      "\n",
      "Summary: His stride was enormous, exaggerated by the fact that the ground seemed to give back considerable\n",
      "        force upward each time one of his feet touched the otherwise mushy surface... Astride his left shoulder\n",
      "        I watched the surroundings.\n",
      "    -- time elapsed: 2.15ms\n",
      "Found tokens (position-index,embedding): [(1, 36024), (31, 7744)]\n",
      "\n",
      "Summary: \"Not all motorbike riders are either St-George or Hell's angels... Nothing will override the statistical evidence  on this!\"\n",
      "    -- time elapsed: 1.27ms\n",
      "Found tokens (position-index,embedding): [(4, 10826), (15, 2586)]\n",
      "\n",
      "Summary: \"You shouldn't have riden your bike on the beach, said the cop, I'm afraid I'll have to take you for a ride!\"\n",
      "    -- time elapsed: 1.141ms\n",
      "Found tokens (position-index,embedding): [(5, 553), (27, 320)]\n",
      "\n",
      "Summary: Canonical representations in 14th to 17th century western iconography often depict knights as \n",
      "        men in armor, who ride horses ... This representation however is neither exclusive, nor generally \n",
      "        accurate. The image of a knight as a horse-rider in shining armor is only familiar to us, because\n",
      "        it corresponds to what is often depicted in modern novels and other fictional productions.\n",
      "    -- time elapsed: 3.093ms\n",
      "Found tokens (position-index,embedding): [(18, 339), (39, 269)]\n",
      "\n",
      "Summary: Canonical representations in 14th to 17th century western iconography often depict knights as \n",
      "        men in armor, who ride horses ... This representation however is by no means exclusive. The image\n",
      "        of a knight as a surfer in Hawaian shorts and who rides waves is only familiar to us, because it \n",
      "        corresponds to what is often depicted in modern novels and other fictional productions.\n",
      "    -- time elapsed: 2.611ms\n",
      "Found tokens (position-index,embedding): [(18, 339), (43, 601)]\n"
     ]
    }
   ],
   "source": [
    "## Text input w/ case-insensitive tokenizer: clip.tokenize()\n",
    "\n",
    "consize = 90                                     # context size\n",
    "\n",
    "def pattern_index_in(summary: str, toi: str) -> list:\n",
    "    '''\n",
    "    Finds indices for occurences of toi in summary.\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(summary)          # tokenize + misc possible conditionals (e.g. tagging, parse-tree position, etc.)\n",
    "    pattern = re.compile(r'^.*' + re.escape(toi) + r'.*$',re.I)   # toi, token of interest\n",
    "    indices = [tokens.index(i) for i in tokens if pattern.match(i)]\n",
    "    return indices\n",
    "\n",
    "def find_embeddings(summary: str, toi: str) -> None:\n",
    "    starttime = time.time()\n",
    "    indices = pattern_index_in(summary,toi)\n",
    "    embedding = [int(clip.tokenize(summary,consize)[0][idx+1]) for idx in indices]\n",
    "    endtime = time.time()\n",
    "    print(f\"\\nSummary: {summary}\\n    -- time elapsed: {round((endtime - starttime)*1000,3)}ms\")\n",
    "    print(f\"Found tokens (position-index,embedding): {list(zip(indices,embedding))}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "toi = \"cover\" \n",
    "print(f\"\\nCLIP contextualized embedding for '{toi}':\\n========================\")\n",
    " \n",
    "text = 'Her long sleeve covered the wound completely. Nobody could possibly unmask her cover-up, she thought.'\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "text = '\"The proposal ought to have been covering the keypoints in question!\", he said non-plused.'\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "toi = \"ride\"\n",
    "print(f\"\\nCLIP contextualized embedding for '{toi}':\\n========================\")\n",
    "\n",
    "text = 'I am a poor lonesome cowbow, riding in the sunset...'\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "text = '''His stride was enormous, exaggerated by the fact that the ground seemed to give back considerable\n",
    "        force upward each time one of his feet touched the otherwise mushy surface... Astride his left shoulder\n",
    "        I watched the surroundings.'''\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "text = '\"Not all motorbike riders are either St-George or Hell\\'s angels... Nothing will override the statistical evidence  on this!\"'\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "text = '\"You shouldn\\'t have riden your bike on the beach, said the cop, I\\'m afraid I\\'ll have to take you for a ride!\"'\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "text = '''Canonical representations in 14th to 17th century western iconography often depict knights as \n",
    "        men in armor, who ride horses ... This representation however is neither exclusive, nor generally \n",
    "        accurate. The image of a knight as a horse-rider in shining armor is only familiar to us, because\n",
    "        it corresponds to what is often depicted in modern novels and other fictional productions.'''\n",
    "find_embeddings(text, toi)\n",
    "\n",
    "text = '''Canonical representations in 14th to 17th century western iconography often depict knights as \n",
    "        men in armor, who ride horses ... This representation however is by no means exclusive. The image\n",
    "        of a knight as a surfer in Hawaian shorts and who rides waves is only familiar to us, because it \n",
    "        corresponds to what is often depicted in modern novels and other fictional productions.'''\n",
    "find_embeddings(text, toi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc3cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python3.7.1 (clip1.0)",
   "language": "python",
   "name": "clip1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
